{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reber grammar classification problem\n",
    "This problem has a very simple solution using classical techniques but we are going to attempt to do it using RNNs.  \n",
    "First let's import the libraries that we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reber grammar\n",
    "Reber grammar can be defined by the following graph\n",
    "![Reber Graph](https://www.willamette.edu/~gorr/classes/cs449/figs/reber.gif)  \n",
    "Simply you traverse the graph, whenever you meet a conjunction you choose a path randomly with equal probability and append the character of the chosen edge to the result string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global graph, we'll need it again\n",
    "GRAPH = [\n",
    "    [(1,5),('T','P')],\n",
    "    [(1,2),('S','X')],\n",
    "    [(3,5),('S','X')],\n",
    "    [(6,),('E')],\n",
    "    [(3,2),('V','P')],\n",
    "    [(4,5),('V','T')]\n",
    "]\n",
    "\n",
    "# function for generating a reber sentence\n",
    "def gen_reber():\n",
    "    seq = \"B\"\n",
    "    index = 0\n",
    "    while index != 6:\n",
    "        target_indices = GRAPH[index][0]\n",
    "        target_chars = GRAPH[index][1]\n",
    "        index = np.random.choice(target_indices)\n",
    "        seq += target_chars[0] if target_indices[0] == index else target_chars[1]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTSSSXXVPSE'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_reber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedded Reber\n",
    "Very similar, however, it uses multiple embedded reber graphs as shown in the picture.  \n",
    "![embedded reber graph](https://www.willamette.edu/~gorr/classes/cs449/figs/embreber.gif)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate embedded reber sentence\n",
    "def embedded_reber():\n",
    "    ## using lambda functions to make all elements of the graph callable\n",
    "    graph = [\n",
    "        [(1,2),(lambda: 'T',lambda: 'P')],\n",
    "        [(3,),(gen_reber,)],\n",
    "        [(4,),(gen_reber,)],\n",
    "        [(5,),(lambda: 'T',)],\n",
    "        [(5,),(lambda: 'P',)],\n",
    "        [(6,),(lambda: 'E',)]\n",
    "    ]\n",
    "    \n",
    "    seq = \"B\"\n",
    "    index = 0\n",
    "    while index != 6:\n",
    "        target_indices = graph[index][0]\n",
    "        target_chars = graph[index][1]\n",
    "        index = np.random.choice(target_indices)\n",
    "        \n",
    "        ## notice that here I call the function, in case it is a simple character it is returned\n",
    "        ## because I used lambda functions above\n",
    "        seq += target_chars[0]() if target_indices[0] == index else target_chars[1]()\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTBTXSETE'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_reber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BTBPVPSETE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_reber(word):\n",
    "    if word[0] != 'B':\n",
    "        return False\n",
    "    node = 0    \n",
    "    for c in word[1:]:\n",
    "        transitions = GRAPH[node]\n",
    "        try:\n",
    "            node = transitions[0][transitions[1].index(c)]\n",
    "        except ValueError:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_embedded_reber(word):\n",
    "    return len(word) > 8 and word[:2] in {\"BP\", \"BT\"} and word[-2:] in {\"PE\", \"TE\"} and is_reber(word[2:-2])\n",
    "\n",
    "embedded_reber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_embedded_reber(\"BPBTSXSEPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random():\n",
    "    ## let's use the same characters used in the graph and not all characters.\n",
    "    ## I am doing so to confuse the model a bit, not to make the pattern too obvious.\n",
    "    chars = [\"T\", \"E\", \"P\", \"B\", \"X\", \"V\", \"S\"]\n",
    "    length = np.random.randint(10, 20)\n",
    "    return ''.join(np.random.choice(chars) for i in range(length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VPXESVPXVTXTTSEPT'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am keeping the zero to the padding token,\n",
    "# usually I would use a keras tokenizer and fit it to the data, but I don't really want to now\n",
    "tokenizer = {\"T\": 1, \"E\": 2, \"P\": 3, \"B\": 4, \"X\": 5, \"V\": 6, \"S\": 7}\n",
    "\n",
    "# let's create a generator function that returns a batch of examples\n",
    "def batch_generator(batch_size=32):\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for i in range(batch_size):\n",
    "            # the batch doesn't have to be balanced, but you can change that if you want.\n",
    "            # I am using randomness to choose the inbalance of the batch\n",
    "            if np.random.choice([0, 1]):\n",
    "                seq = list(map(lambda x: tokenizer[x], embedded_reber()))\n",
    "                batch_x.append(seq)\n",
    "                batch_y.append(1)\n",
    "            else:\n",
    "                random = gen_random()\n",
    "                seq = list(map(lambda x: tokenizer[x], random))\n",
    "                # now this is random which doesn't mean it is not embedded reber\n",
    "                # we need to check for our selves\n",
    "                if is_embedded_reber(random):\n",
    "                    batch_x.append(seq)\n",
    "                    batch_y.append(1)\n",
    "                else:\n",
    "                    batch_x.append(seq)\n",
    "                    batch_y.append(0)\n",
    "        # make the batch sequences of the same length by applying padding\n",
    "        batch_x = tf.keras.preprocessing.sequence.pad_sequences(batch_x)\n",
    "        shape = batch_x.shape\n",
    "        # reshpe it to be (batch_size, seq_length, 1)\n",
    "        batch_x = batch_x.reshape((shape[0], shape[1], 1))\n",
    "        # make y a vector not a row matrix\n",
    "        batch_y = np.array(batch_y).reshape(-1, 1)\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 19, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = batch_generator()\n",
    "x, y = next(g)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, None, 16)          1152      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,097\n",
      "Trainable params: 4,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this line is for jupyter because the graph keeps some stuff when you run the cell multiple times\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(16, input_shape=[None, 1], return_sequences=True),\n",
    "    LSTM(16),\n",
    "    Dense(16, activation=LeakyReLU(0.2)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation=LeakyReLU(0.2)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation=LeakyReLU(0.2)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "32/32 [==============================] - 2s 54ms/step - loss: 0.6921 - accuracy: 0.5010 - val_loss: 0.6860 - val_accuracy: 0.5303\n",
      "Epoch 2/16\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.6797 - accuracy: 0.6201 - val_loss: 0.6580 - val_accuracy: 0.8057\n",
      "Epoch 3/16\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.6369 - accuracy: 0.6680 - val_loss: 0.5600 - val_accuracy: 0.8115\n",
      "Epoch 4/16\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.5326 - accuracy: 0.7627 - val_loss: 0.3966 - val_accuracy: 0.8535\n",
      "Epoch 5/16\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.4182 - accuracy: 0.8447 - val_loss: 0.2841 - val_accuracy: 0.9092\n",
      "Epoch 6/16\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3192 - accuracy: 0.8857 - val_loss: 0.1973 - val_accuracy: 0.9307\n",
      "Epoch 7/16\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.2817 - accuracy: 0.9141 - val_loss: 0.1731 - val_accuracy: 0.9355\n",
      "Epoch 8/16\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 0.2414 - accuracy: 0.9180 - val_loss: 0.1655 - val_accuracy: 0.9346\n",
      "Epoch 9/16\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.2034 - accuracy: 0.9365 - val_loss: 0.1445 - val_accuracy: 0.9531\n",
      "Epoch 10/16\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 0.2063 - accuracy: 0.9316 - val_loss: 0.1388 - val_accuracy: 0.9502\n",
      "Epoch 11/16\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1481 - accuracy: 0.9619 - val_loss: 0.1290 - val_accuracy: 0.9551\n",
      "Epoch 12/16\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.1498 - accuracy: 0.9551 - val_loss: 0.1252 - val_accuracy: 0.9570\n",
      "Epoch 13/16\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1403 - accuracy: 0.9541 - val_loss: 0.1164 - val_accuracy: 0.9590\n",
      "Epoch 14/16\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1143 - accuracy: 0.9678 - val_loss: 0.0983 - val_accuracy: 0.9629\n",
      "Epoch 15/16\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.1174 - accuracy: 0.9678 - val_loss: 0.0789 - val_accuracy: 0.9756\n",
      "Epoch 16/16\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.0962 - accuracy: 0.9775 - val_loss: 0.0637 - val_accuracy: 0.9824\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    batch_generator(),\n",
    "    epochs=16,\n",
    "    steps_per_epoch=32,\n",
    "    validation_data=batch_generator(),\n",
    "    validation_steps=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a preprocess function for future inference\n",
    "def preprocess(word):\n",
    "    seq = np.array(list(map(lambda x: tokenizer[x], word)))\n",
    "    seq = seq.reshape(seq.shape + (1, ))\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-b29135c888c0>:4: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's test it on two examples\n",
    "tests = [preprocess(embedded_reber()), preprocess(gen_random())]\n",
    "tests = pad_sequences(tests)\n",
    "model.predict_classes(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It got them right!\n",
    "That's that for this challange, I guess."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
